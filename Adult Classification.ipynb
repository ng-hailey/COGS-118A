{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88e1356",
   "metadata": {},
   "source": [
    "# Adult Classification #\n",
    "\n",
    "**Adult Dataset**:https://archive.ics.uci.edu/dataset/2/adult\n",
    "\n",
    "**Goal**: Predict whether an adult's income exceedds $50,000/year based on census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33cf708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6807af",
   "metadata": {},
   "source": [
    "## Step 1: Import and Adjust Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19851de1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'adult.data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Manually adding headers to the data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m headers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorkclass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFnlwgt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEducation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEducation-Num\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMarital-Status\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOccupation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelationship\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSex\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapital-Gain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapital-Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHours-Per-Week\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNative-Country\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncome\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madult.data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, names \u001b[38;5;241m=\u001b[39m headers)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#Preview the data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'adult.data.csv'"
     ]
    }
   ],
   "source": [
    "#Manually adding headers to the data\n",
    "headers = [\"Age\", \"Workclass\", \"Fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\", \"Hours-Per-Week\", \"Native-Country\", \"Income\"]\n",
    "\n",
    "data = pd.read_csv('adult.data.csv', header = None, names = headers)\n",
    "\n",
    "#Preview the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c858f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually drop redundant/unncessary rows of data\n",
    "\n",
    "#Fnlwgt (\"final weight\" - the number of people the census thinks this represents) is irrelevant\n",
    "data = data.drop(\"Fnlwgt\", axis=1)\n",
    "#Education-num is the same as education, but on a numerical scale\n",
    "data = data.drop(\"Education\", axis=1)\n",
    "#Marital status and relationship are redundant \n",
    "data = data.drop(\"Relationship\", axis=1)\n",
    "#Race is a sensitive topic and assigning numerical values to it would be insensitive\n",
    "data = data.drop(\"Race\", axis=1)\n",
    "#The same logic applies for native country\n",
    "data = data.drop(\"Native-Country\", axis=1)\n",
    "data = data.drop(\"Capital-Gain\", axis=1)\n",
    "data = data.drop(\"Capital-Loss\", axis=1)\n",
    "\n",
    "#Preview the updated data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff68956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess datatypes in the dataset - mix of strings and ints\n",
    "print(data.dtypes)\n",
    "\n",
    "#Check for any NaN values in the dataset - no NaNs!\n",
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844443bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the label encoder function of SKlearn to do convert categorical data\n",
    "#via ordinal encoding\n",
    "\n",
    "lb_Make = LabelEncoder()\n",
    "data[\"Workclass\"] = lb_Make.fit_transform(data[\"Workclass\"])\n",
    "data[\"Marital-Status\"] = lb_Make.fit_transform(data[\"Marital-Status\"])\n",
    "data[\"Occupation\"] = lb_Make.fit_transform(data[\"Occupation\"])\n",
    "#Female = 0, Male = 1\n",
    "data[\"Sex\"] = lb_Make.fit_transform(data[\"Sex\"])\n",
    "#<=50K = 0, >=50K = 1\n",
    "data[\"Income\"] = lb_Make.fit_transform(data[\"Income\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d067476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cut down size to only 10,000 random samples\n",
    "data = data.sample(n=10000).reset_index(drop=True)\n",
    "\n",
    "#Convert dataset to Numpy array\n",
    "data = data.values\n",
    "\n",
    "#Preview the data\n",
    "print(data[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6218ce50",
   "metadata": {},
   "source": [
    "## Step 2: Defining Classifiers ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "global count \n",
    "count = 0\n",
    "def draw_heatmap(acc, acc_desc, C_list, character):\n",
    "    global count\n",
    "    plt.figure(figsize = (2,4))\n",
    "    ax = sns.heatmap(acc, annot=True, fmt='.3f', yticklabels=C_list, xticklabels=[])\n",
    "    ax.collections[0].colorbar.set_label(\"Accuracy\")\n",
    "    ax.set(ylabel='$'  + character + '$')\n",
    "    plt.title(acc_desc + ' w.r.t $' + character + '$')\n",
    "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "    plt.show()\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80484937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree (DT) Classifier\n",
    "def decision_tree(X_train_val, Y_train_val, X_test, Y_test):\n",
    "    D_list = np.array([1, 2, 3, 4, 5])\n",
    "    parameters = {'max_depth':D_list}\n",
    "    \n",
    "    #Finding the optimal model\n",
    "    classifier_grid = GridSearchCV(DecisionTreeClassifier(criterion=\"entropy\"), parameters, cv=5, return_train_score=True)\n",
    "    \n",
    "    #Fit classifier with training data\n",
    "    classifier_grid.fit(X_train_val, Y_train_val)\n",
    "    \n",
    "    #Training + validation accuracy\n",
    "    DT_train_acc = classifier_grid.cv_results_['mean_train_score']\n",
    "    DT_val_acc = classifier_grid.cv_results_['mean_test_score']\n",
    "\n",
    "    #Train + test model with best parameter\n",
    "    D_best_param = classifier_grid.best_params_['max_depth']\n",
    "    classifier_test = DecisionTreeClassifier(max_depth=D_best_param, criterion=\"entropy\")\n",
    "    classifier_test.fit(X_train_val, Y_train_val)\n",
    "    \n",
    "    for i,j in enumerate(D_list):\n",
    "        if j == D_best_param:\n",
    "            DT_best_train_acc = DT_train_acc[i]\n",
    "    \n",
    "    #Test accuracy\n",
    "    DT_test_acc = classifier_test.score(X_test,Y_test)\n",
    "            \n",
    "    #Training + validation accuracy heatmaps\n",
    "    draw_heatmap(DT_train_acc.reshape(5,1), 'DT train accuracy', D_list, 'D')\n",
    "    draw_heatmap(DT_val_acc.reshape(5,1), 'DT val accuracy', D_list, 'D')\n",
    "    \n",
    "    return DT_test_acc, DT_best_train_acc, D_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Classifier\n",
    "def knn_classifier(X_train_val, Y_train_val, X_test, Y_test):\n",
    "    K_list = np.array([1, 2, 3, 4, 5, 6])\n",
    "    parameters = {'n_neighbors':K_list}\n",
    "    \n",
    "    #Finding the optimal model\n",
    "    classifier_grid = GridSearchCV(KNeighborsClassifier(), parameters, cv=5, return_train_score=True)\n",
    "    \n",
    "    #Fit classifier with training data\n",
    "    classifier_grid.fit(X_train_val, Y_train_val)\n",
    "    \n",
    "    #Training + validation accuracy\n",
    "    KNN_train_acc = classifier_grid.cv_results_['mean_train_score']\n",
    "    KNN_val_acc = classifier_grid.cv_results_['mean_test_score']\n",
    "\n",
    "    #Train + test model with best parameter\n",
    "    K_best_param = classifier_grid.best_params_['n_neighbors']\n",
    "    classifier_test2 = KNeighborsClassifier(n_neighbors=K_best_param)\n",
    "    classifier_test2.fit(X_train_val,Y_train_val)\n",
    "    \n",
    "    for i,j in enumerate(K_list):\n",
    "        if j == K_best_param:\n",
    "            KNN_best_train_acc = KNN_train_acc[i]\n",
    "    \n",
    "    #Test accuracy\n",
    "    KNN_test_acc = classifier_test2.score(X_test,Y_test)\n",
    "    \n",
    "    #Training + validation accuracy heatmaps\n",
    "    draw_heatmap(KNN_train_acc.reshape(6,1), 'KNN train accuracy', K_list, 'K')\n",
    "    draw_heatmap(KNN_val_acc.reshape(6,1), 'KNN val accuracy', K_list, 'K')\n",
    "    \n",
    "    return KNN_test_acc, KNN_best_train_acc, K_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf2eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "def random_forest(X_train_val, Y_train_val, X_test, Y_test):\n",
    "    D_list = np.array([1, 2, 3, 4, 5])\n",
    "    parameters = {'max_depth':D_list}\n",
    "    \n",
    "    #Finding the optimal model\n",
    "    classifier_grid = GridSearchCV(RandomForestClassifier(criterion=\"entropy\"), parameters, cv=5, return_train_score=True)\n",
    "    \n",
    "    #Fit classifier with training data\n",
    "    classifier_grid.fit(X_train_val, Y_train_val)\n",
    "    \n",
    "    #Training + validation accuracy\n",
    "    RF_train_acc = classifier_grid.cv_results_['mean_train_score']\n",
    "    RF_val_acc = classifier_grid.cv_results_['mean_test_score']\n",
    "\n",
    "    #Train + test model with best parameter\n",
    "    D_best_param = classifier_grid.best_params_['max_depth']\n",
    "    classifier_test1 = RandomForestClassifier(max_depth=D_best_param, criterion=\"entropy\")\n",
    "    classifier_test1.fit(X_train_val, Y_train_val)\n",
    "    \n",
    "    for i,j in enumerate(D_list):\n",
    "        if j == D_best_param:\n",
    "            RF_best_train_acc = RF_train_acc[i]\n",
    "    \n",
    "    #Test accuracy\n",
    "    RF_test_acc = classifier_test1.score(X_test,Y_test)\n",
    "    \n",
    "    #Training + validation accuracy heatmaps\n",
    "    draw_heatmap(RF_train_acc.reshape(5,1), 'RF train accuracy', D_list, 'K')\n",
    "    draw_heatmap(RF_val_acc.reshape(5,1), 'RF val accuracy', D_list, 'K')\n",
    "    \n",
    "    return RF_test_acc, RF_best_train_acc, D_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e3645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Linear Classification\n",
    "def svm_linear(X_train_val, Y_train_val, X_test, Y_test):\n",
    "    classifier = svm.SVC(kernel = 'linear')\n",
    "    C_list = [10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 1]\n",
    "    parameters = {'C': C_list}\n",
    "    \n",
    "    #Finding the optimal model\n",
    "    clf = GridSearchCV(classifier, parameters, return_train_score=True, cv=5)\n",
    "    \n",
    "    #Fit classifier with training data\n",
    "    clf.fit(X_train_val, Y_train_val)\n",
    "    \n",
    "    #Training + validation accuracy\n",
    "    SVM_train_acc = clf.cv_results_['mean_train_score']\n",
    "    SVM_val_acc = clf.cv_results_['mean_test_score']\n",
    "\n",
    "    #Find the optimal C parameter \n",
    "    C_best_param = clf.best_params_['C']\n",
    "    optimal_classifier = svm.SVC(kernel = 'linear', C=C_best_param)\n",
    "    SVM_best_train_acc = None\n",
    "    \n",
    "    for i,j in enumerate(C_list):\n",
    "        if j == C_best_param:\n",
    "            SVM_best_train_acc = SVM_train_acc[i]\n",
    "            break\n",
    "            \n",
    "    #Redefining the optimal classifier\n",
    "    optimal_classifier.fit(X_train_val, Y_train_val)\n",
    "    \n",
    "    #Test accuracy\n",
    "    SVM_test_acc = optimal_classifier.score(X_test, Y_test)\n",
    "\n",
    "    #Training + validation accuracy heatmaps\n",
    "    draw_heatmap(SVM_train_acc.reshape(-1,1), 'SVM Linear train accuracy', C_list, 'C')\n",
    "    draw_heatmap(SVM_val_acc.reshape(-1,1), 'SVM Linear val accuracy', C_list, 'C')\n",
    "\n",
    "    return SVM_test_acc, SVM_best_train_acc, C_best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e74d68",
   "metadata": {},
   "source": [
    "## Step 3: Perform Classification ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e0664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#List partitions: 20/80, 50/50, 80/20\n",
    "partition_nums = [0.2, 0.5, 0.8]\n",
    "\n",
    "#Prepare result tables: test accuracy, best training accuracy, best parameter\n",
    "test_acc_table = np.zeros((3,4))\n",
    "best_train_table = np.zeros((3,4))\n",
    "best_param_table = np.zeros((3,4))\n",
    "\n",
    "#Perform classification over number of trials\n",
    "\n",
    "for i, partition in enumerate(partition_nums):\n",
    "    print(\"Partition: \", partition)\n",
    "    \n",
    "    #Prepare arrays to hold the test accuracies from each classifier\n",
    "    DT_test_acc = []\n",
    "    KNN_test_acc = []\n",
    "    RF_test_acc = []\n",
    "    SVM_test_acc = []\n",
    "    \n",
    "    #3 trials per partition\n",
    "    num_trials = 3\n",
    "    \n",
    "    #Begin conducting trials and performing classification\n",
    "    for trial in range(num_trials):\n",
    "        #Shuffle the data to ensure randomization\n",
    "        np.random.shuffle(data)\n",
    "        \n",
    "        #Find the partition point\n",
    "        breakNum = int(partition * len(data))\n",
    "        \n",
    "        #Make the training and testing sets\n",
    "        X_train_val = data[:breakNum,:-1]\n",
    "        Y_train_val = data[:breakNum,-1]\n",
    "        X_test = data[breakNum:,:-1]\n",
    "        Y_test = data[breakNum:,-1]\n",
    "        \n",
    "        print(X_train_val.shape, X_test.shape, Y_train_val.shape, Y_test.shape)\n",
    "        \n",
    "        #Now we call the classifier functions defined in Step 2\n",
    "        #After each function call, append the test accuracy to the appropriate array\n",
    "        \n",
    "        #Decision Tree Classifier\n",
    "        test_acc, DT_best_train, DT_best_param = decision_tree(X_train_val, Y_train_val, X_test, Y_test)\n",
    "        DT_test_acc.append(test_acc)\n",
    "        \n",
    "        #KNN Classifier\n",
    "        test_acc, KNN_best_train, KNN_best_param = knn_classifier(X_train_val, Y_train_val, X_test, Y_test)\n",
    "        KNN_test_acc.append(test_acc)\n",
    "        \n",
    "        #Random Forest Classifier\n",
    "        test_acc, RF_best_train, RF_best_param = random_forest(X_train_val, Y_train_val, X_test, Y_test)\n",
    "        RF_test_acc.append(test_acc)\n",
    "        \n",
    "        #SVM linear Classifier\n",
    "        test_acc, SVM_best_train, SVM_best_param = svm_linear(X_train_val, Y_train_val, X_test, Y_test)\n",
    "        SVM_test_acc.append(test_acc)\n",
    "        \n",
    "        print(\"Trial # = \", trial + 1)\n",
    "\n",
    "    #Calculate the average test accuracies across the trials for each classifier\n",
    "    DT_avg_test_acc = sum(DT_test_acc)/num_trials\n",
    "    KNN_avg_test_acc = sum(KNN_test_acc)/num_trials\n",
    "    RF_avg_test_acc = sum(RF_test_acc)/num_trials\n",
    "    SVM_avg_test_acc = sum(SVM_test_acc)/num_trials\n",
    "    \n",
    "    #Fill in test_acc table with average test accuracy for each classifier\n",
    "    test_acc_table[i,0] = DT_avg_test_acc\n",
    "    test_acc_table[i,1] = KNN_avg_test_acc\n",
    "    test_acc_table[i,2] = RF_avg_test_acc\n",
    "    test_acc_table[i,3] = SVM_avg_test_acc\n",
    "    \n",
    "    #Fill in best_train table with best training accuracy for each classifier\n",
    "    best_train_table[i,0] = DT_best_train\n",
    "    best_train_table[i,1] = KNN_best_train\n",
    "    best_train_table[i,2] = RF_best_train\n",
    "    best_train_table[i,3] = SVM_best_train\n",
    "    \n",
    "    #Fill in best_param table with best hyperparameter for each classifier\n",
    "    best_param_table[i,0] = DT_best_param\n",
    "    best_param_table[i,1] = KNN_best_param\n",
    "    best_param_table[i,2] = RF_best_param\n",
    "    best_param_table[i,3] = SVM_best_param\n",
    "    \n",
    "    #Print out average test accuracies for the trials for each classifier\n",
    "    print(\"Avg Test Accuracy for Decision Tree (DT) = \", DT_avg_test_acc)\n",
    "    print(\"Avg Test Accuracy for KNN = \", KNN_avg_test_acc)\n",
    "    print(\"Avg Test Accuracy for Random Forest = \", RF_avg_test_acc)\n",
    "    print(\"Avg Test Accuracy for SVM Linear = \", SVM_avg_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf808fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out tables\n",
    "print(\"Y-axis: partitions = 20/80, 50/50, 80/20\")\n",
    "print(\"X-axis: classifiers = Decision Tree, KNN, Random Forest, SVM Linear\")\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "print(\"Test Accuracy Table\")\n",
    "print(test_acc_table)\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "print(\"Best Training Accuracy Table\")\n",
    "print(best_train_table)\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "print(\"Best Parameter Table\")\n",
    "print(best_param_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df437e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
